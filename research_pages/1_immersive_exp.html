<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Immersive Wave Experimentation (IWE): Cloaking, Holography, and Cloning — Jack Li</title>
  <meta name="description" content="Story and key results behind immersive wave experimentation: mixing physical and virtual domains, removing boundary artefacts (MDD), broadband cloaking and holography, and acoustic cloning.">
  <link rel="stylesheet" href="../styles.css">
</head>
<body>
  <main id="wrapper">
    <section>
      <p>
        <a href="../index.html#research">← Back to Main Page</a>
      </p>
      
      <h2>Immersive Wave Experimentation (IWE): Cloaking, Holography, and Cloning</h2>
      
<!-- finished on 15, Nov, 2025, to be polished with my own words and deep understanding later on to motivate myself a bit on this research direction. -->

      <p>
        If you have ever clapped in a small room or dropped a pebble in a bucket, you know the problem: waves bounce off the boundaries. In a real ocean, a quiet field, or outer space, waves can radiate away seemingly “to infinity.” In a lab, they hit walls and come right back. Those echoes contaminate measurements, shrink the space where we can trust the results, and make it hard to study how waves actually behave in the wild.
      </p>

      <p>
        We decided not to fight that limitation with bigger rooms or thicker foam. Instead, we taught the walls to help. Our approach blends a physical experiment with a virtual world in real time. We call this Immersive Boundary Conditions (IBCs). Think of it as a portal: the lab stays small, but the waves inside it “believe” they are in an unbounded environment because the boundaries actively behave as if the outside extends forever. 
        IBC let us switch off a lab’s walls, make objects acoustically invisible, project virtual objects into reality, and even clone the scattering fingerprint of anything we like—on demand and across a broad range of frequencies.
        For more details, see 
        <a href="https://mge.ethz.ch/research/centre-immersive.html" target="_blank" rel="noopener noreferrer">
          Centre for Immersive Wave Experimentation
        </a>
      
      </p>
      

    <!-- <figure class="zoomable">  zoomable figure here " id="wrapper" -->
      <div class="figure-grid" style="width: min(1260px, 92vw);">
        <figure>
          <!-- Figure suggestion: “WaveLab” or similar lab photo -->
          <a href="1_immersive_exp/ETH_WaveLab.png" target="_blank" rel="noopener noreferrer">
            <img src="1_immersive_exp/ETH_WaveLab.png" alt="Acoustic wave lab with arrays, scanning LDV, and waveguide rig" style="aspect-ratio: auto; object-fit: contain;">
          </a>
          <figcaption>
            <strong>ETH WaveLab</strong><br>
            This is a versatile acoustic research facility where experiments at kHz frequencies are performed in compact, 
            reflective spaces. For example, a 2D “sandwich” waveguide is used to simulate layered media and study broadband acoustic propagation, 
            while a large water tank (measuring 3.1 × 4.5 × 2.7 m) serves as a quasi–3D environment 
            for immersive experiments that bridge the frequency gap between laboratory and field scales. 
            In addition, a state‐of‐the‐art LDV robot is employed to scan rock cubes, enabling precise elastic wave measurements at the surfaces of solid media. 
            Together, these setups exemplify how ETH WaveLab uses Immersive Boundary Conditions (IBCs) and Multidimensional Deconvolution (MDD) to 
            virtually remove lab boundaries, thereby canceling unwanted reflections, rendering objects invisible (cloaking), projecting virtual scatterers (holography),
            and even cloning full broadband scattering responses on demand.
          </figcaption>
        </figure>
      </div>

<strong>Related:</strong>  Landrø &amp; Favretto‐Cristini, <a href="https://physics.aps.org/articles/v11/71" target="_blank" rel="noopener noreferrer">Acoustic Physics</a> (2018).

    </section>

  
    <!-- Section 1: The challenge and the promise -->
    <section id="challenge">
      <h3>1) The challenge: small labs, bouncing waves</h3>

      <p>
       In the wild—open water, the sky, or space—waves radiate away and their energy leaves the scene, but in tanks, rooms, and waveguides they ricochet off walls,
        polluting measurements and burying subtle effects like weak scattering or fine phase shifts. Anechoic chambers try to tame those echoes with foam wedges, 
        yet their effectiveness collapses at low frequency because the wedge depth must scale with wavelength, 
        making low‑kHz setups impractically large; in solid 3D volumes for full elastic P–S studies, operating around 1–20 kHz means wavelengths approach
         the sample size, so boundary energy can dominate the interior wavefield. Reduced‑size labs (kHz acoustics, plates, rooms, waveguides)
          thus suffer reflections that mask the very scattering we want to observe; passive absorbers are bulky and narrowband, 
          while classic active cancellation degrades off‑axis and at higher frequencies. This is compounded by a scale gap: seismology and many field scenarios live
           below ~100 Hz, whereas conventional lab acoustics often push to hundreds of kHz or MHz to keep echoes at bay, making it hard to transfer 
           insights—especially for frequency‑dependent phenomena like attenuation, dispersion, anisotropy, and nonlinearity that do not upscale reliably. 

      </p>

      <!-- Flex container for side-by-side media -->
      <div class="figure-grid" style="display: flex; flex-wrap: nowrap; gap: 1rem; justify-content: center; width: min(1080px, 92vw);">
        <!-- Left figure: PNG image -->
        <figure style="flex: 1; min-width: 300px; max-width: 480px; margin: 0 auto; padding-top: 25px;">
          <a href="1_immersive_exp/schematic_rigid_wall.png" target="_blank" rel="noopener noreferrer">
            <img src="1_immersive_exp/schematic_rigid_wall.png" alt="Schematic: Small lab boundaries affecting wave propagation" style="width: 100%; aspect-ratio: auto; object-fit: contain;">
          </a>
          <figcaption>
            <strong>Wave-physics laboratories</strong><br>
                     Size limitation causes wall-boundary reflections and restricts the natural propagation of waves.<br>
      </figcaption>
        </figure>

 
        <!-- Right figure: MOV movie (Autoplay version) -->
        <figure style="flex: 1; min-width: 530px; max-width: 680px; ">
          <video autoplay muted loop playsinline style="width: 220.5%; height: auto; object-fit: cover; 
          margin-top: -233px; 
          margin-right: -80px; 
          margin-bottom: -10px; 
          margin-left: -780px;">
            <source src="1_immersive_exp/schematic_rigid_wall_reflection_movie_2.mp4" type="video/quicktime">
            Your browser does not support the video tag.
          </video>
          <figcaption>
            <strong>Boundary reflections in low-frequency experiments</strong><br>
            Laboratories often suffer because their size is too small to simulate natural, non-boundary wave propagation. As shown, these boundary reflections strongly interfere with the interior experimental domain where scattering is being studied.
          </figcaption>
        </figure>

      </div>

      <p>
        <strong>Related:</strong> Xun Li, <a href="https://doi.org/10.3929/ethz-b-000573091" target="_blank" rel="noopener noreferrer"><em>Elastic immersive wave experimentation</em></a> – Doctoral Thesis, ETH Zurich, 2022.
      </p>

   <!-- Right figure: MOV movie with Button to Play <button onclick="document.getElementById('myVideo').play()" style="margin-top: 10px; padding: 8px 16px; font-size: 16px;">Play Video</button> -->
    

    </section>
  

  <!-- Section: Immersive Boundary Conditions (IBCs) Concept -->
  <section id="ibc-concept">
    <h3>2) Immersive Boundary Conditions: Active wave energy cancellation</h3>

    <p>
      In size-limited laboratories, reflections from rigid boundaries often obscure the waves scattered by interior objects.
      One elegant solution is the deployment of active sources around the physical experimental domain.
      In an ideal implementation, the active boundary sources generate counter-waves that completely cancel the outgoing wavefield.
      As a result, the only waves visible in the experiment are those related to the interior scatterers, free from unwanted
      boundary effects. This clear separation is crucial for accurate measurement and analysis. Also, in this way,
       the experiment is virtually "immersed" into a much larger space where the propagation characteristics are as if no rigid boundaries were present. 
       In effect, the physical wave propagation mirrors the behavior of waves in an idealized, boundary-free environment.
      </p>


     <div class="figure-grid" style="display: flex; flex-wrap: nowrap; gap: 1rem; justify-content: center; width: min(1260px, 92vw);">
        <!-- single line figure: MOV movie (Autoplay version) -->
        <figure style="flex: 1;">
          <video autoplay muted loop playsinline style="width: 120%; height: auto; object-fit: cover; 
          margin-top: -220px; 
          margin-right: -80px; 
          margin-bottom: -10px; 
          margin-left: -260px;">
            <source src="1_immersive_exp/boundary_cancel_waves.mp4" type="video/quicktime">
            Your browser does not support the video tag.
          </video>
          <figcaption>
            <strong>Active wave cancellation</strong><br>
            The concept of using active sources to cancel outgoing waves in a wave experiment.
          </figcaption>
        </figure>

      </div>

      <p>
        The active sources can create a virtual environment by emitting waves designed not only to cancel naturally outgoing waves through carefully controlled emissions,
         but also to actively represent interactions between a physical experiment and an artificial virtual domain. In this configuration, the physical wave propagation 
         experiment becomes seamlessly connected to an extended or virtual environment—the emitted waves embodying the desired properties of an unbounded, 
         immersive domain while the propagating waves remain explicit in the physical realm.
      </p>

      <div class="figure-grid" style="width: min(860px, 72vw);">
        <figure>
          <!-- Figure suggestion: “WaveLab” or similar lab photo -->
          <a href="1_immersive_exp/schematic_immersive_wave_exp.PNG" target="_blank" rel="noopener noreferrer">
            <img src="1_immersive_exp/schematic_immersive_wave_exp.PNG" alt="Schematic of immersive wave experiment" style="aspect-ratio: auto; object-fit: contain;">
          </a>
          <figcaption>
            <strong>Immersive Wave Experimentation Concept</strong><br>
            By employing active sources to both cancel outgoing waves and emit tailored waves, we simulate an unbounded or extended experimental domain.
            This setup enables the representation of interactions between the physical and virtual environments, allowing for an accurate study of wave propagation 
            and scattering phenomena as if the laboratory boundaries were non-existent.
          </figcaption>
        </figure>
      </div>

<p>
  Simulation movies of this approach strikingly illustrate that, with active cancellation, the laboratory’s confined physical space transforms
   into an immersive experimental domain. This environment faithfully replicates real-world scattering phenomena and supports broadband measurements without
    the distortions typically caused by reflections.
</p>

<div class="figure-grid" style="display: flex; flex-wrap: nowrap; gap: 1rem; justify-content: center; width: min(1560px, 112vw);">
  <!-- single line figure: MOV movie (Autoplay version) -->
  <figure style="flex: 1;">
    <video autoplay muted loop playsinline style="width: 100%; height: auto; object-fit: cover; 
    margin-top: 0px; 
    margin-right: 0px; 
    margin-bottom: 0px; 
    margin-left: 0px;">
      <source src="1_immersive_exp/whole_immersive_movie.mp4" type="video/quicktime">
      Your browser does not support the video tag.
    </video>
    <figcaption>
      <strong>Immersive Wave Experimentation</strong><br>
      Waves propagating in the physical setup correspond exactly to the physical component of an expansive, virtual wave environment.
    </figcaption>
  </figure>
</div>
  </section>


<section id="ibc-acoustic">
  <h3>3) Immersive Boundary Conditions (IBCs): Real-time, broadband acoustic implementation</h3>
  <p>
    IBCs let us virtually replace what lies beyond a control surface by extrapolating measurements from an outer recording ring to an inner emitting ring in real time. 
    In our implementation, pressure and particle velocity measured on a closed outer surface are used to predict the incident wave field on the inner boundary using
     representation theorems together with precomputed Green’s functions as “wave kernels” to extrapolate the field. This approach provides deterministic,
      broadband, and global control at low latency, independent of frequency or incident angle. With low-latency FPGAs (no more than 200 μs compute time), the extrapolated wave field is immediately re-injected into the lab so that waves cross seamlessly between the physical 
      and virtual domains. This real-time prediction method cancels unwanted boundary reflections, enabling complex effects such as cloaking and holography without
       any prior knowledge of the incident wave.
  </p>

  <div class="figure-grid" style="width: min(1260px, 92vw);">
    <figure>
      <a href="1_immersive_exp/ibc_acoustic.png" target="_blank" rel="noopener noreferrer">
        <img src="1_immersive_exp/ibc_acoustic.png" alt="Two-ring IBC schematic: outer recording and inner emitting arrays" style="aspect-ratio: auto; object-fit: contain;">
      </a>

      <figcaption>
        <strong>Acoustic Immersive Wave Experimentation</strong><br>
        Schematic showing the outer recording and inner emitting arrays and the real-time extrapolation loop.
      </figcaption>
    </figure>
  </div>

  <p>
    <strong>Related:</strong> Becker et al., <a href="https://doi.org/10.1103/PhysRevX.8.031011" target="_blank" rel="noopener noreferrer">PRX</a>, 2018.
  </p>
</section>


<!-- Section 4: Broadband cloaking (real-time, unknown sources) -->
<section id="cloaking">
  <h3>4) Cloaking: Perfect invisibility at all angles without prior source knowledge</h3>

<p>
  Think Harry Potter’s invisibility cloak: light bends around him and no one sees a thing. Doing that for light in open space is
  famously hard—an optical cloak would need to steer every color and every angle around an object and still deliver the wavefront to
  the observer “on time.” Passive materials run into causality and dispersion limits, so practical optical cloaks tend to be
  narrowband, angle/polarization‑limited, or confined to special geometries.
</p>

<p>
  For sound, we take a different route: instead of hiding the object, we hide its acoustic signature. Using interior boundary control
  (IBC), an outer ring of microphones listens and a real‑time extrapolation predicts the wavefield at an inner ring of loudspeakers.
  Those speakers emit a secondary field that cancels the object’s scattering—including multipath from the lab boundaries—without any
  prior model of the incident sound. Even moving or unknown sources can be cloaked. 
</p>


  <!-- Flex container for side-by-side media -->
  <div class="figure-grid" style="display: flex; flex-wrap: nowrap; gap: 1rem; justify-content: center; width: min(1080px, 92vw);">
    <!-- Left figure: Conceptual/simulation schematic (cloaking only) -->
    <figure style="flex: 1; min-width: 300px; max-width: 480px; margin: 0 auto; padding-top: 0px;">
      <a href="1_immersive_exp/Cloaking_Schematic.PNG" target="_blank" rel="noopener noreferrer">
        <img src="1_immersive_exp/Cloaking_Schematic.PNG"
             alt="Active acoustic cloaking concept: microphones predict and speakers cancel the object’s scattering"
             style="aspect-ratio: auto; object-fit: contain;">
      </a>
      <figcaption>
        <strong>Active cloaking concept</strong><br>
        A primary source generates the incident field; control
        loudspeakers add a secondary field that suppresses the object’s scattering. Control inputs are computed in
        real time by forward wavefield extrapolation from microphone measurements.
      </figcaption>
    </figure>

    <!-- Right figure: Experimental setup -->
    <figure style="flex: 1; min-width: 515px; max-width: 680px;">
      <a href="1_immersive_exp/Cloaking_Exp_setup.png" target="_blank" rel="noopener noreferrer">
        <img src="1_immersive_exp/Cloaking_Exp_setup.png"
             alt="Experimental setup: dual microphone rings feed a low-latency FPGA controller driving an inner loudspeaker ring"
             style="aspect-ratio: auto; object-fit: contain;">
      </a>
      <figcaption>
        <strong>2D Experimental platform for active acoustic cloaking</strong><br>
        (A) schematic and (B) photograph. Two circular arrays of
        114 microphones each record the pressure field (C). An FPGA-based low-latency computational and control unit. (D) performs real-time
        extrapolation to drive the control loudspeakers (E, F). 
      </figcaption>
    </figure>
  </div>

<p>
  What does “invisible” look like in practice? We cloaked a quasi‑rigid circular scatterer (diameter 12.6 cm) inside a 2D waveguide by surrounding it with 20 control 
  loudspeakers. A moving broadband primary field was synthesized by eight sources along a 96° arc. With the cloak off, both back‑ and forward‑scattering are visible;
   with the cloak on, the measured total field matches the “no‑object” reference—even at later times when wall echoes dominate. Finite‑element simulations closely 
   reproduce the measurements. Across ~3.5 octaves (up to ~8.7 kHz), the mean scattered intensity at the outer array is reduced by about −8.4 dB and the angular 
   scattering pattern is uniformly suppressed, demonstrating all‑angle, source‑agnostic cloaking.
</p>

<div class="figure-grid" style="width: min(880px, 92vw);">
  <figure>
    <a href="1_immersive_exp/cloaking_result.jpg" target="_blank" rel="noopener noreferrer">
      <img src="1_immersive_exp/cloaking_result.jpg"
           alt="Cloaking results: cloak off/on/reference snapshots, measured and simulated fields, and frequency and angle-dependent scattered intensity reductions"
           style="aspect-ratio: auto; object-fit: contain;">
    </a>
    <figcaption>
      <strong>2D Cloaking of a Rigid Object in a Waveguide</strong><br>
      (A–C) Experimental setup and snapshots; (D–F) measured fields at the outer microphone ring; (G–I) simulations of the same;
      (J–K) scattered fields without/with cloak; (L) reduction in scattered acoustic intensity versus frequency; (M) angular distribution of scattered intensity.
    </figcaption>
  </figure>
</div>

  <p><strong>Examples adapted from </strong> Becker et&nbsp;al., <a href="https://doi.org/10.1126/sciadv.abi9627" target="_blank" rel="noopener noreferrer">Science Advances</a>, 2021.</p>
</section>



<!-- Section 5: Holography (virtual imprints) -->
<section id="holography">
  <h3>5) Holography: Cheat an observer with virtual imprints</h3>

  <p>
    Picture the classic sci‑fi hologram: a lifelike person appears in front of you to talk, even though they’re far away. 
    The trick is that your eyes receive the exact wavefronts they would have seen if that person were really there. 
    Acoustic holography does the same for sound. We make microphones and loudspeakers conspire so an observer hears precisely the scattered field a 
    chosen object would produce—even when that object isn’t present. By predicting, in real time, the wavefield on a sound‑transparent inner surface and driving 
    collocated monopole+dipole emission (two close loudspeaker rings), we “imprint” a virtual object into the lab. To microphones (and your ears),
     the illusion is indistinguishable from the real scatterer. Note that we use the same real-time extrapolation as for cloaking, but now to 
     recreate the object’s scattered field rather than cancel it.
  </p>

  <!-- Flex container for side-by-side media -->
  <div class="figure-grid" style="display: flex; flex-wrap: nowrap; gap: 1rem; justify-content: center; width: min(1080px, 92vw);">
    <!-- Left figure: Conceptual/simulation schematic (holography only) -->
    <figure style="flex: 1; min-width: 300px; max-width: 480px; margin: 0 auto; padding-top: 0px;">
      <a href="1_immersive_exp/holography_Schematic.PNG" target="_blank" rel="noopener noreferrer">
        <img
          src="1_immersive_exp/holography_Schematic.PNG"
          alt="Simulation concept: active acoustic holography creates the scattered field of an object that is not physically present using real-time extrapolation"
          style="aspect-ratio: auto; object-fit: contain;"
        >
      </a>
      <figcaption>
      <strong>Simulations demonstrating active holography</strong><br>
       A primary source emits an initial wavefield (C), and active control sources create a hologram of an object that 
       is not physically present (D). Control inputs are obtained by real‑time forward extrapolation from control sensor 
       measurements.
      </figcaption>
    </figure>
    <!-- Right figure: Experimental holography results -->
    <figure style="flex: 1; min-width: 710px; max-width: 680px;">
      <a href="1_immersive_exp/holography_Results.PNG" target="_blank" rel="noopener noreferrer">
        <img
          src="1_immersive_exp/holography_Results.PNG"
          alt="Experimental creation of an acoustic hologram: setup, snapshots, and comparison of angular scattering for physical versus virtual scatterers"
          style="aspect-ratio: auto; object-fit: contain;"
        >
      </a>
      <figcaption>
      <strong>Creation of acoustic holograms at a sound‑transparent surface</strong><br>
       (A) Experimental setup and snapshots at 2.5&nbsp;ms and corresponding simulations.
       (B) Reference with a physical object inside the waveguide. (C) Active hologram: control loudspeakers enabled, no physical object. 
       (D) Angular distribution of scattered intensity for a physical (E) and virtual (F) scatterer. 
      </figcaption>
    </figure>
  </div>

  <p><strong>Examples adapted from </strong> Becker et&nbsp;al., <a href="https://doi.org/10.1126/sciadv.abi9627" target="_blank" rel="noopener noreferrer">Science Advances</a>, 2021.</p>
  <p><strong>See also:</strong> van Manen et&nbsp;al., <a href="https://doi.org/10.1121/1.4919366" target="_blank" rel="noopener noreferrer">JASA</a>, 2015 (original theory); Börsing et&nbsp;al., <a href="https://doi.org/10.1103/PhysRevApplied.12.024011" target="_blank" rel="noopener noreferrer">Phys. Rev. Applied</a>, 2019 (1D demonstration).</p>
</section>



<section id="cloning">
  <h3>6) Acoustic Cloning: A Digital Twin Enabling Scatters Like the Real Object</h3>

  <p>
    Acoustic cloning is a powerful, two‑step strategy that ultimately creates a “digital twin” of a real scatterer—that is, a non‑physical replica that 
    reproduces the same acoustic scattering characteristics as the original object. In our system,
     the fundamental idea is to record the true scattering response of an object embedded in a well‑defined experimental environment and 
     then use that retrieved information to “play back” the object’s influence on any arbitrary incident wavefield. 
     This entire process unfolds in two primary steps:
  </p>
  
    <!-- PAD THE BELOW WITH INCIDENT -->
    <li>
      <strong>Retrieval of the real scatterer’s properties through Multidimensional Deconvolution (MDD).</strong>
    </li>
    <li>
      <strong>Holographic reconstruction of the scatterer’s imprint via immersive holographic techniques.</strong>
    </li>
  <!-- <hr> -->

  <h4>Step 1: Acquiring the Scatterer’s Green’s Functions Using MDD</h3>

  <p>
    The first stage of acoustic cloning involves illuminating the real scatterer with controlled broadband signals. 
    During this illumination, wavefields are recorded on a closed, sound‑transparent receiver aperture surrounding the scatterer.
     By applying the method of <a href="../mdd_work.html">multidimensional deconvolution (MDD)</a>, the experimental data are post‑processed
      to extract the scatterer’s full set of Green’s functions. These functions characterize how the acoustic wavefield propagates from the 
      sources through the medium and scatters off the object. Crucially, the MDD approach removes any extraneous contributions—such as multiple
       reflections and boundary effects—that would otherwise “pollute” the clean scattering signature of the object.
  </p>

  <p>
    By eliminating the boundary‑induced reverberations, the resulting Green’s functions represent the object’s response under ideal radiation conditions. In other words, we retrieve the “pure” scattering characteristics as if the object were in an unbounded space.
  </p>

  <figure style="width:min(960px,92vw);margin:1rem auto;">
    <a href="1_immersive_exp/Experiment_Geometry.PNG" target="_blank" rel="noopener noreferrer">
      <img src="1_immersive_exp/Experiment_Geometry.PNG" alt="2D waveguide with outer recording ring SO and inner ring SI; microphones and sources for MDD and holography" style="width:100%;height:auto;">
    </a>
    <figcaption>
      Setup: A two‐dimensional acoustic waveguide with two sound‑transparent rings. The outer ring (SO) records the wavefield, while the inner ring (SI) is used both for capturing data in the MDD step and for emitting signals in the holography step. A rigid boundary encloses the domain, but its imprint is carefully removed by MDD.
    </figcaption>
  </figure>

  <p>
    Once the raw data have been decomposed into their incident and outgoing components (a key process in MDD), a set of coupled Fredholm integral equations is formulated. Solving these equations in a least‑squares sense enables us to reconstruct the Green’s functions that reveal the object’s scattering behavior.
  </p>

  <p>
    <strong>What Does This Mean Practically?</strong><br>
    No prior knowledge of the object’s material properties, geometry, or source signature is required. Every detail is captured from the measurement data, making the approach remarkably general and robust even in reverberant laboratory environments.
  </p>

  <figure style="width:min(720px,92vw);margin:1rem auto;">
    <a href="1_immersive_exp/motivate_Cloning_GFs.PNG" target="_blank" rel="noopener noreferrer">
      <img src="1_immersive_exp/motivate_Cloning_GFs.PNG" alt="Motivation: why we modify Green’s functions to achieve cloning under radiation conditions" style="width:100%;height:auto;">
    </a>
    <figcaption>
      Why change the Green’s functions? By removing unwanted boundary effects, the true scattering contribution of the object can be isolated and later digitally “pasted” back into the experimental domain with perfect fidelity.
    </figcaption>
  </figure>

  <p>
    In summary, after illuminating the scatterer and applying MDD, we obtain a set of Green’s functions that isolate and capture the object’s scattering response. For example, by subtracting the homogeneous (direct wave) component, only the contributions from the scatterer remain.
  </p>

  <figure style="width:min(960px,92vw);margin:1rem auto;">
    <a href="1_immersive_exp/obtained_Green_function.PNG" target="_blank" rel="noopener noreferrer">
      <img src="1_immersive_exp/obtained_Green_function.PNG" alt="Subset of scattering Green’s functions retrieved by MDD with boundary multiples removed" style="width:100%;height:auto;">
    </a>
    <figcaption>
      Scattering Green’s functions retrieved by MDD (subset: G(φO, t | φI = 0°)). Boundary‑related multiples are removed to leave only the pure scattering contribution from the object—ready for holographic replay.
    </figcaption>
  </figure>

  <!-- <hr> -->

  <h4>Step 2: Reconstructing the Scatterer with Real‑Time Holography</h4>
  <p>
    With the scatterer’s Green’s functions in hand, the next step is to recreate the object’s acoustic imprint within the lab—even after the physical object has been removed. This is achieved using a holographic reconstruction process:
  </p>

  <ol>
    <li>
      <strong>Deactivate the Object:</strong> The physical scatterer is removed from the experimental domain.
    </li>
    <li>
      <strong>Replay Green’s Functions:</strong> The previously retrieved Green’s functions are used as signal templates to drive an array of transducers on the inner ring (SI). These transducers emit time‑varying monopole and dipole signals derived from the Green’s functions.
    </li>
  </ol>

  <p>
    The resulting wavefield, excited by these precisely calibrated signals, reproduces all scattering phenomena—including multiple interactions between the incident wave and the scatterer’s numerical imprint—exactly as if the original object were still in place.
  </p>

  <p>
    <strong>A Few Notable Features of the Holographic Clone:</strong><br>
    • <em>Broadband Robustness:</em> The hologram responds correctly for any incident broadband wavefield.<br>
    • <em>Versatility:</em> Whether the original scatterer is circular, square, or a complex cross, the holographic reconstruction faithfully reproduces its scattering features.<br>
    • <em>Real‑Time Reproduction:</em> The reconstruction occurs with low latency; the echo and interference patterns build up in real time.
  </p>

  <figure style="width:min(960px,92vw);margin:1rem auto;">
    <a href="1_immersive_exp/cloning_results_one.PNG" target="_blank" rel="noopener noreferrer">
      <img src="1_immersive_exp/cloning_results_one.PNG" alt="Real vs cloned scattering: hologram off/on and comparison for circle, square, cross" style="width:100%;height:auto;">
    </a>
    <figcaption>
      Validation: Comparison of the wavefield with holography off (empty waveguide) and with it on (holographic reconstruction active). Subtracting the homogeneous field shows that the scattered field matches the responses of the original scatterer across various shapes.
    </figcaption>
  </figure>

  <!-- <hr> -->

  <h4>Augmenting the Digital Twin: Beyond Cloning</h4>

  <p>
    An especially exciting aspect of this approach is that the twin is defined digitally. Once the scatterer’s Green’s functions have been retrieved, they can be manipulated arbitrarily. For instance, you can:
  </p>

  <ul>
    <li><strong>Rotate or Translate the Clone:</strong> Configure different spatial arrangements without physically modifying the original object.</li>
    <li><strong>Scale Amplitudes:</strong> Introduce directional gain to alter the amplitude of scattering in specific directions.</li>
    <li><strong>Modify Impedance or Transparency:</strong> Create “acoustic cyborgs” that interact with real wavefields while exhibiting non‑physical (or enhanced) behaviors.</li>
  </ul>

  <p>
    These digital modifications can be made prior to playback through the holographic emitter. This opens up applications in:
  </p>

  <ul>
    <li>
      Enhanced metamaterial designs, where unit cells can be cloned and arranged without the challenges of physical manufacturing.
    </li>
    <li>
      Virtual acoustic models, allowing simulation of an object’s response in various environments or configurations—thereby enabling rapid prototyping and testing.
    </li>
  </ul>

  <figure style="width:min(960px,92vw);margin:1rem auto;">
    <a href="1_immersive_exp/cloning_results_two.PNG" target="_blank" rel="noopener noreferrer">
      <img src="1_immersive_exp/cloning_results_two.PNG" alt="Augmented clones: directional gain for a circle and numerical rotation for a square" style="width:100%;height:auto;">
    </a>
    <figcaption>
      Augmented cloning: The left image shows a circular clone programmed to exhibit directional gain—amplifying reflections and yielding more transparent transmissions. The right image shows a square clone digitally rotated to replicate a physically rotated object. These examples highlight the potential to recreate and enhance scattering behavior beyond what is physically possible.
    </figcaption>
  </figure>

  <!-- <hr> -->

<h4>Wandering Thoughts</h4>

<p>
  Acoustic cloning feels a bit like moving from making models of the world to making models that <em>act back</em> on the world. We’re not copying atoms or materials; we’re copying how an object <em>behaves</em> when sound meets it—its scattering fingerprint—and then letting that behavior live on without the object. In that sense, the “clone” is less a replica and more a portable skill: a learned interaction you can carry to new places, switch on and off, or even bend to your will.
</p>

<p>
  Why does this matter? Because interaction is often what we care about most. Acoustic cloning captures the full give-and-take between a wavefield and an object, across angles, frequencies, and multiple reflections. That changes how we prototype and experiment. Instead of machining many variants of a metamaterial, you clone the unit cell once and iterate digitally. Instead of hunting for anechoic spaces, you bring your own “infinite” lab with you. Instead of waiting for large simulations, you measure, learn, and replay—on demand.
</p>

<p>
  The ripple effects extend beyond research. Imagine:
</p>

<ul>
  <li><strong>Design at the speed of thought:</strong> Try, measure, clone, tweak—repeat. Turn days of fabrication into minutes of iteration.</li>
  <li><strong>Acoustic heritage and education:</strong> Preserve the “sound” of instruments, halls, or artifacts and let students interact with them anywhere.</li>
  <li><strong>Personalized spaces:</strong> Car cabins, classrooms, and headsets that remix their acoustic behavior in real time for comfort, clarity, or immersion.</li>
  <li><strong>Metamaterials, demystified:</strong> Clone a unit cell, tile it virtually, and explore nonphysical variants (gain, custom directivity) before ever building.</li>
</ul>

<p>
  Of course, power invites responsibility. If we can project acoustic “truths” into a room, we should mark what’s real and what’s rendered. Standards for safe output levels, disclosure in public spaces, and reproducible reporting will help keep the tech trustworthy. And we should keep sight of the limits: a clone is only as good as the bandwidth and geometry it learned from.
</p>

<p>
  Personally, what excites me most is how <em>tangible</em> this feels. It’s not just a simulation on a screen; it’s physics you can stand inside. The jump from measuring a thing to <em>becoming</em> it—acoustically—changes how we ask questions. We can now ask “What if this object were rotated, softer, or directional?” and get the answer back from the lab in real time. That’s a different kind of conversation with nature—and one I suspect we’re only beginning to have.
</p>

  <p>
    <p><strong>Examples adapted from </strong> Müller, J., Becker, T. S., Li, X., et al., <a href="https://doi.org/10.1103/PhysRevApplied.20.064014" target="_blank" rel="noopener noreferrer">Phys. Rev. Applied.</a> (2023)
  </p>
  
  <p>
   <p><strong> For further reading </strong> into the theory and implementation of MDD , please refer to the <a href="../mdd_work.html">MDD work page</a>.
  </p>
</section>

    <!-- Section 7: Applications and prospects -->
<section id="applications">
  <h3>7) Applications: Where Immersive Wave Experiments Make a Difference</h3>

  <p>
    What excites me about immersive wave experimentation is how it transforms a cluttered, small lab into an imaginative playground—a place where ideas usually confined to simulations take on life. Imagine switching off walls, dropping virtual media into the mix, and crafting physical/virtual hybrids that behave like materials we haven’t even built yet. In our lab, we talk about these experiments with energy and enthusiasm, and here are just a few examples of where this technology makes waves—literally and figuratively:
  </p>

  <ul>
    <li>
      <strong>Mixing Real and Virtual Wave Scattering.</strong> 
      Prototype complex samples by combining a tangible physical core with a flexible virtual surround. Measure once, then remix shapes, impedances, and arrangements digitally—no need for re-machining after every tweak.
    </li>
    <li>
      <strong>Virtually Created Media and Metamaterials.</strong> 
      Build phononic crystals and metamaterials in software; let your lab “feel” them in real time. Explore phenomena like gain media or parity–time (PT) symmetric responses that would be impossible to achieve with traditional passive materials.
    </li>
    <li>
      <strong>Independent Car Audio.</strong> 
      Imagine creating multiple immersive zones—one per passenger—by surrounding each listener with compact arrays that couple to their own virtual acoustic space. The result? A personalized soundscape delivered without overwhelming the cabin.
    </li>
    <li>
      <strong>VR-Room Acoustics for Learning and Play.</strong> 
      Step into “impossible” rooms that morph from the echo of a cathedral to the silence of an anechoic box. Students and audiences can walk through these dynamic environments while the same wavefield is measured in real time.
    </li>
    <li>
      <strong>Hybrid Metamaterials and Phononic Crystals.</strong> 
      Embrace the best of both worlds: keep one physical unit cell while virtually tiling the surrounding lattice. This fusion lets you experiment with sweeping changes in geometry, bias, and non‑physical parameters (like gain, loss, or asymmetry) between measurements—without having to rebuild your setup.
    </li>
    <li>
      <strong>Nonlinear, Time‑Varying Media & PT Symmetry.</strong> 
      Inject controlled nonlinearity or time modulation directly into your numerical models, then tether them to the real wavefield. Mix a virtual gain medium with a physical lossy element to craft direction‑dependent transmission and absorption effects that passive samples simply can’t sustain.
    </li>
    <li>
      <strong>Digital Twins and Cloning.</strong> 
      Capture a scatterer’s Green’s functions and replay them holographically. In doing so, you can “clone” the object on demand—rotating it, moving it, or digitally modifying its impedance at the drop of a hat.
    </li>
    <li>
      <strong>Low‑Frequency, Broadband Rigs and Collocated Apertures.</strong>
      Future-ready modular 3D arrays can span entire rooms (≈2.7 × 3.1 × 4.5 m) and dive deeper into lower frequency regimes without needing bulky absorbers. By collocating emit/record surfaces, we can suppress higher‑order coupling, making first‑order Green’s components the stars of the show—thanks to low‑latency control.
    </li>
    <li>
      <strong>Open Recipes for Safety and Reproducibility.</strong>
      The idea is to eventually share kernels, calibration sets, and level‑safe presets so that others can reproduce—and responsibly deploy—immersive experiments. In this way, the lab isn’t just editing waves; it’s editing the rules of the game.
    </li>
  </ul>

  <p>
    Beyond these immediate applications, immersive wave experimentation opens doors to entirely new realms of physics and wave control strategies. By blending physical and virtual worlds, our lab becomes more than a measuring instrument—it evolves into a dynamic canvas on which we sculpt the flow of energy and matter.
  </p>

  <p>
    There’s also a deeper scientific edge to this work. Elastic immersive wave experimentation unlocks a low‑frequency window (≈1–20&nbsp;kHz in solids) that conventional small labs struggle to explore. In this regime, wavelengths are comparable to sample sizes, providing an ideal testbed for homogenization theory, scale‑bridging ideas, and advanced techniques like time reversal and virtual source methods. This is where the lab not only pushes the envelope of wave manipulation but also pioneers potential breakthroughs in seismic imaging and non‑destructive testing.
  </p>

  <div class="figure-grid" style="width: min(760px, 92vw);">
    <figure>
      <!-- Replace the src below with your attached metamaterials figure -->
      <a href="1_immersive_exp/meta_materials.PNG" target="_blank" rel="noopener noreferrer">
        <img src="1_immersive_exp/meta_materials.PNG" alt="Metamaterial: one physical unit cell in the lab, virtually tiled to build a phononic crystal/metamaterial lattice" style="aspect-ratio: auto; object-fit: contain;">
      </a>
      <figcaption>
        Hybrid metamaterials via immersion: (b) shows one physical unit cell in the lab, virtually tiled to emulate an infinite lattice as depicted in graph (a). This virtual domain can host unconventional properties—negative parameters, gain, or PT symmetry—while the physical measurements provide the essential ground truth.
      </figcaption>
    </figure>
  </div>

  <p style="text-align: right;">
    Graph adapted from: <a href="https://doi.org/10.3929/ethz-b-000573091" target="_blank" rel="noopener noreferrer"><em>Elastic Immersive Wave Experimentation</em></a> (ETH Zurich, 2022).
  </p>

  <p>
    In short, by merging the tangible with the virtual, we are not just capturing wave phenomena—we are reinventing them. Every experiment becomes a leap into uncharted territory, where physics, engineering, and creativity converge to redefine what’s possible.
  </p>
</section>




<!-- Section 8: Key references -->
    <section id="references">
      <h3>References</h3>


        <!-- 2022: Doctoral Thesis by Xun Li -->
        <li>
          <p>
            Li, X. (2022). <em>Elastic immersive wave experimentation</em> [Doctoral thesis, ETH Zurich]. 
            <a href="https://doi.org/10.3929/ethz-b-000573091" target="_blank" rel="noopener noreferrer">
              DOI: 10.3929/ethz-b-000573091
            </a>
          </p>
        </li>

      <!-- 2021: Closed-aperture unbounded acoustics experimentation (JASA) -->
        <li>
          <p>
            Li, X., Becker, T., Ravasi, M., Robertsson, J., &amp; van Manen, D.-J. (2021). 
            <em>Closed-aperture unbounded acoustics experimentation using multidimensional deconvolution</em>. 
            <strong>Journal of the Acoustical Society of America, 149</strong>(3), 1813–1828. 
            <a href="https://doi.org/10.1121/10.0003706" target="_blank" rel="noopener noreferrer">
              DOI: 10.1121/10.0003706
            </a>
          </p>
        </li>

        <!-- 2021: Broadband acoustic invisibility in Science Advances -->
        <li>
          <p>
            Becker, T. S., van Manen, D.-J., Haag, T., Bärlocher, C., Li, X., Börsing, N., Curtis, A., Serra‑Garcia, M., &amp; Robertsson, J. O. A. (2021). 
            <em>Broadband acoustic invisibility and illusions</em>. 
            <strong>Science Advances, 7</strong>(37), eabi9627. 
            <a href="https://doi.org/10.1126/sciadv.abi9627" target="_blank" rel="noopener noreferrer">
              DOI: 10.1126/sciadv.abi9627
            </a>
          </p>
        </li>

        <!-- 2018: Immersive wave propagation experimentation (Phys. Rev. X) -->
        <li>
          <p>
            Becker, T. S., van Manen, D.-J., Donahue, C. M., Bärlocher, C., Börsing, N., Broggini, F., Haag, T., Robertsson, J. O. A., Schmidt, D. R., Greenhalgh, S. A., &amp; Blum, T. E. (2018). 
            <em>Immersive wave propagation experimentation: Physical implementation and one‑dimensional acoustic results</em>. 
            <strong>Physical Review X, 8</strong>(3), 031011. 
            <a href="https://doi.org/10.1103/PhysRevX.8.031011" target="_blank" rel="noopener noreferrer">
              DOI: 10.1103/PhysRevX.8.031011
            </a>
          </p>
        </li>

        <!-- 2023: Acoustic cloning (Phys. Rev. Applied) -->
        <li>
          <p>
            Müller, J., Becker, T. S., Li, X., Aichele, J., Serra‑Garcia, M., Robertsson, J. O. A., &amp; van Manen, D.-J. (2023). 
            <em>Acoustic cloning: Creating digital twins of acoustic scatterers in immersive wave experiments</em>. 
            <strong>Physical Review Applied, 20</strong>(6), 064014. 
            <a href="https://doi.org/10.1103/PhysRevApplied.20.064014" target="_blank" rel="noopener noreferrer">
              DOI: 10.1103/PhysRevApplied.20.064014
            </a>
          </p>
        </li>



 
    </section>




</main>



<!-- foot notes -->
  <footer class="site-footer">
    <div class="container">
      <small>© <span id="year"></span> Jack Li • Edinburgh, UK</small>
    </div>
  </footer>

  <script>
    (function () {
      const y = document.getElementById('year');
      if (y) y.textContent = new Date().getFullYear();
    })();
  </script>



<!-- Control the basic of zoom and pan -->

  <!-- Zoom and pan script -->
<script>
  document.addEventListener('DOMContentLoaded', () => {
    const wrapper = document.getElementById('wrapper');
    if (!wrapper) return;

    // Create an inner zoom target so we can translate + scale cleanly
    let zoomInner = document.getElementById('zoom-inner');
    if (!zoomInner) {
      zoomInner = document.createElement('div');
      zoomInner.id = 'zoom-inner';
      // Move all children into zoomInner
      while (wrapper.firstChild) zoomInner.appendChild(wrapper.firstChild);
      wrapper.appendChild(zoomInner);
    }

    // Base state
    let scale = 1;
    let tx = 0; // translation x (px)
    let ty = 0; // translation y (px)
    const ZOOM_STEP = 1.05;
    const MIN_SCALE = 0.5;
    const MAX_SCALE = 3;

    // Always use a fixed origin so every section scales the same
    zoomInner.style.transformOrigin = 'center center';
    zoomInner.style.willChange = 'transform';

    const clamp = (v, lo, hi) => Math.max(lo, Math.min(v, hi));
    const applyTransform = () => {
      zoomInner.style.transform = `translate(${tx}px, ${ty}px) scale(${scale})`;
    };

    // Zoom helpers
    const zoomBy = (factor) => {
      const newScale = clamp(scale * factor, MIN_SCALE, MAX_SCALE);
      if (newScale === scale) return;
      scale = newScale;
      applyTransform();
    };

    // Ctrl/Cmd + mouse wheel zoom (uniform origin)
    window.addEventListener('wheel', (event) => {
      if (!event.ctrlKey && !event.metaKey) return;
      event.preventDefault(); // prevent browser zoom
      const factor = event.deltaY < 0 ? ZOOM_STEP : 1 / ZOOM_STEP;
      zoomBy(factor);
    }, { passive: false });

    // Ctrl/Cmd + '+' or '-' (and numpad) for keyboard zoom
    window.addEventListener('keydown', (event) => {
      // Don't hijack typing in form fields/contenteditable
      const el = event.target;
      const isEditable = el && (el.isContentEditable ||
        ['INPUT', 'TEXTAREA', 'SELECT'].includes(el.tagName));
      if (isEditable) return;

      const hasModifier = event.ctrlKey || event.metaKey;
      if (!hasModifier) return;

      let handled = false;

      if (event.code === 'Equal' || event.code === 'NumpadAdd' || event.key === '+') {
        zoomBy(ZOOM_STEP);
        handled = true;
      } else if (event.code === 'Minus' || event.code === 'NumpadSubtract' || event.key === '-') {
        zoomBy(1 / ZOOM_STEP);
        handled = true;
      } else if (event.code === 'Digit0' || event.code === 'Numpad0' || event.key === '0') {
        scale = 1;
        tx = 0;
        ty = 0;
        handled = true;
      }

      if (handled) {
        event.preventDefault(); // prevent browser’s own page zoom
        applyTransform();
      }
    });

    // Optional: Space + drag to pan when zoomed
    let isPanning = false;
    let startX = 0, startY = 0, startTx = 0, startTy = 0;

    const startPan = (clientX, clientY) => {
      isPanning = true;
      startX = clientX;
      startY = clientY;
      startTx = tx;
      startTy = ty;
      document.body.style.cursor = 'grabbing';
    };

    const movePan = (clientX, clientY) => {
      if (!isPanning) return;
      const dx = clientX - startX;
      const dy = clientY - startY;
      tx = startTx + dx;
      ty = startTy + dy;
      applyTransform();
    };

    const endPan = () => {
      isPanning = false;
      document.body.style.cursor = '';
    };

    window.addEventListener('mousedown', (e) => {
      // Only pan when Space is held
      if (!e.target || !e.isTrusted) return;
      if (e.button === 0 && (e.getModifierState && e.getModifierState(' '))) {
        e.preventDefault();
        startPan(e.clientX, e.clientY);
      }
    });

    window.addEventListener('mousemove', (e) => movePan(e.clientX, e.clientY));
    window.addEventListener('mouseup', endPan);
    window.addEventListener('mouseleave', endPan);

    // For touchpads that send 'gesturestart/gesturechange' (Safari), keep uniform scaling
    window.addEventListener('gesturestart', (e) => e.preventDefault());
    window.addEventListener('gesturechange', (e) => {
      e.preventDefault();
      // e.scale is relative to the start of the gesture; convert to incremental factor
      // Keep it gentle to avoid jumps
      const factor = clamp(e.scale, 0.95, 1.05);
      zoomBy(factor);
    }, { passive: false });

    // Initial apply
    applyTransform();
  });
</script>



</body>
</html>



